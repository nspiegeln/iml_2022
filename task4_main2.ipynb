{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task4_main2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu3d/hz1vypY7LFHryhvkM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nspiegeln/iml_2022/blob/main/task4_main2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "9T_5cUxPdqb5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDyTG0FmgJEq",
        "outputId": "2c1c4a45-d5ab-4d9e-920e-3cb5fa0a55fa"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"gdrive/MyDrive/task4/\"\n",
        "df_train= pd.read_csv(path + \"train_features.csv\")\n",
        "df_train_labels= pd.read_csv(path + \"train_labels.csv\")\n",
        "df_pretrain= pd.read_csv(path +\"pretrain_features.csv\")\n",
        "df_pretrain_labels= pd.read_csv(path +\"pretrain_labels.csv\")\n",
        "df_test= pd.read_csv(path +\"test_features.csv\")"
      ],
      "metadata": {
        "id": "eNN4gItzd9Jj"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_values = df_train.iloc[:,2::]\n",
        "df_pretrain_values = df_pretrain.iloc[:,2::]\n",
        "\n",
        "df_train_lab = df_train_labels.iloc[:,1]\n",
        "df_pretrain_lab = df_pretrain_labels.iloc[:,1]\n",
        "\n",
        "train = df_train_values.values #dataset with known homo-lumo gap\n",
        "pretrain = df_pretrain_values.values #dataset with known lumo value\n",
        "\n",
        "train_labels = df_train_lab.values\n",
        "pretrain_labels = df_pretrain_lab.values"
      ],
      "metadata": {
        "id": "w3TcQUVjd_nz"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_pretrain, x_pretest, y_pretrain, y_pretest = train_test_split(pretrain, pretrain_labels, test_size = 0.1)\n",
        "print(x_pretrain.shape[0], x_pretrain.shape[1])\n",
        "print(x_pretest.shape[0], x_pretest.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czaE5YppeA0t",
        "outputId": "79395f06-7536-444b-ccee-b7fa5e2b0976"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45000 1000\n",
            "5000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get device\n",
        "def get_default_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_default_device()"
      ],
      "metadata": {
        "id": "L9QjuXA0eudl"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # PRE-PROCESSING of features according to autoencoder\n",
        "# # pre-train set\n",
        "\n",
        "# #hyperparameters\n",
        "# dim = 50\n",
        "# epochs = 30 # best results for epochs= 5 for dim = 200\n",
        "\n",
        "# # create encoder and decoder to reduce dimensionality of features\n",
        "# class Autoencoder(Model):\n",
        "#   def __init__(self, dim):\n",
        "#     super(Autoencoder, self).__init__()\n",
        "#     self.dim = dim   \n",
        "#     self.encoder = tf.keras.Sequential([\n",
        "#       layers.Flatten(),\n",
        "#       layers.Dense(dim, activation='relu'),\n",
        "#     ])\n",
        "#     self.decoder = tf.keras.Sequential([\n",
        "#       layers.Dense(1000, activation='sigmoid')\n",
        "#     ])\n",
        "\n",
        "#   def call(self, x):\n",
        "#     encoded = self.encoder(x)\n",
        "#     decoded = self.decoder(encoded)\n",
        "#     return decoded\n",
        "\n",
        "# autoencoder = Autoencoder(dim)\n",
        "\n",
        "# autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "# autoencoder.fit(x_pretrain, x_pretrain, epochs=epochs, shuffle=True, validation_data=(x_pretest, x_pretest))"
      ],
      "metadata": {
        "id": "pkiD7zrGeCr0"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # hyperparameters\n",
        "# output_dim = 1\n",
        "# epochs = 5        # doesn't decrease after 5, bad loss\n",
        "# batch_size = 64\n",
        "\n",
        "# # predict one LUMO label\n",
        "# class Predictor(Model):\n",
        "#     def __init__(self, output_dim):\n",
        "#       super(Predictor, self).__init__()\n",
        "#       self.output_dim = output_dim   \n",
        "#       self.predictor = tf.keras.Sequential([\n",
        "#         layers.Flatten(),\n",
        "#         layers.Dense(50, activation='relu'),\n",
        "#         layers.Dense(25, activation='relu'),\n",
        "#         layers.Dense(5, activation='relu'),\n",
        "#         layers.Dense(output_dim, activation='relu'),\n",
        "#       ])\n",
        "\n",
        "#     def call(self, x):\n",
        "#       predicted = self.predictor(x)\n",
        "#       return predicted\n",
        "    \n",
        "# predict = Predictor(output_dim)\n",
        "# predict.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "# predict.fit(z_encoded_pretrain, y_pretrain, batch_size = batch_size, epochs=epochs, shuffle=True) #, validation_data=(x_encoded_pretest, y_pretest))"
      ],
      "metadata": {
        "id": "172ViH8keIdE"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRE-PROCESSING of features according to autoencoder\n",
        "# pre-train set\n",
        "\n",
        "#hyperparameters\n",
        "dim = 200\n",
        "epochs = 100         #resuls still decrease after epoch 100,  @epoch=60 loss=4.7e-3, @epoch=100 loss=3.4e-3\n",
        "batch_size = 64\n",
        "\n",
        "# create encoder and decoder to reduce dimensionality of features\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.dim = dim   \n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(500, activation='relu'),\n",
        "      layers.Dense(dim, activation='relu'),\n",
        "    ])\n",
        "    self.predictor = tf.keras.Sequential([\n",
        "      layers.Dense(100, activation='relu'),\n",
        "      layers.Dense(50, activation='relu'),\n",
        "      layers.Dense(1),\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    predicted = self.predictor(encoded)\n",
        "    return predicted\n",
        "\n",
        "autoencoder = Autoencoder(dim)\n",
        "sgd = SGD(learning_rate=0.1, decay=1e-2, momentum=0.9)\n",
        "autoencoder.compile(optimizer='sgd', loss=losses.MeanSquaredError())\n",
        "\n",
        "autoencoder.fit(x_pretrain, y_pretrain, epochs=epochs, batch_size = batch_size, shuffle=True, validation_data=(x_pretest, y_pretest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h054QOrr2Gay",
        "outputId": "4691789a-be0e-4c57-c618-95d4abb1afa2"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "704/704 [==============================] - 23s 30ms/step - loss: 0.0866 - val_loss: 0.0279\n",
            "Epoch 2/100\n",
            "704/704 [==============================] - 23s 33ms/step - loss: 0.0249 - val_loss: 0.0223\n",
            "Epoch 3/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0200 - val_loss: 0.0219\n",
            "Epoch 4/100\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0173 - val_loss: 0.0171\n",
            "Epoch 5/100\n",
            "704/704 [==============================] - 20s 28ms/step - loss: 0.0156 - val_loss: 0.0181\n",
            "Epoch 6/100\n",
            "704/704 [==============================] - 15s 22ms/step - loss: 0.0143 - val_loss: 0.0140\n",
            "Epoch 7/100\n",
            "704/704 [==============================] - 35s 50ms/step - loss: 0.0133 - val_loss: 0.0132\n",
            "Epoch 8/100\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0125 - val_loss: 0.0154\n",
            "Epoch 9/100\n",
            "704/704 [==============================] - 15s 21ms/step - loss: 0.0118 - val_loss: 0.0120\n",
            "Epoch 10/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0113 - val_loss: 0.0136\n",
            "Epoch 11/100\n",
            "704/704 [==============================] - 20s 28ms/step - loss: 0.0108 - val_loss: 0.0116\n",
            "Epoch 12/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0104 - val_loss: 0.0110\n",
            "Epoch 13/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0100 - val_loss: 0.0121\n",
            "Epoch 14/100\n",
            "704/704 [==============================] - 20s 29ms/step - loss: 0.0097 - val_loss: 0.0155\n",
            "Epoch 15/100\n",
            "704/704 [==============================] - 26s 37ms/step - loss: 0.0094 - val_loss: 0.0105\n",
            "Epoch 16/100\n",
            "704/704 [==============================] - 25s 35ms/step - loss: 0.0091 - val_loss: 0.0098\n",
            "Epoch 17/100\n",
            "704/704 [==============================] - 31s 44ms/step - loss: 0.0088 - val_loss: 0.0096\n",
            "Epoch 18/100\n",
            "704/704 [==============================] - 26s 37ms/step - loss: 0.0086 - val_loss: 0.0095\n",
            "Epoch 19/100\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.0084 - val_loss: 0.0181\n",
            "Epoch 20/100\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0082 - val_loss: 0.0102\n",
            "Epoch 21/100\n",
            "704/704 [==============================] - 23s 32ms/step - loss: 0.0080 - val_loss: 0.0115\n",
            "Epoch 22/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0078 - val_loss: 0.0089\n",
            "Epoch 23/100\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0077 - val_loss: 0.0097\n",
            "Epoch 24/100\n",
            "704/704 [==============================] - 22s 32ms/step - loss: 0.0075 - val_loss: 0.0086\n",
            "Epoch 25/100\n",
            "704/704 [==============================] - 21s 29ms/step - loss: 0.0074 - val_loss: 0.0087\n",
            "Epoch 26/100\n",
            "704/704 [==============================] - 19s 27ms/step - loss: 0.0072 - val_loss: 0.0106\n",
            "Epoch 27/100\n",
            "704/704 [==============================] - 22s 31ms/step - loss: 0.0071 - val_loss: 0.0092\n",
            "Epoch 28/100\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0070 - val_loss: 0.0088\n",
            "Epoch 29/100\n",
            "704/704 [==============================] - 20s 28ms/step - loss: 0.0069 - val_loss: 0.0081\n",
            "Epoch 30/100\n",
            "704/704 [==============================] - 22s 31ms/step - loss: 0.0067 - val_loss: 0.0088\n",
            "Epoch 31/100\n",
            "704/704 [==============================] - 27s 38ms/step - loss: 0.0066 - val_loss: 0.0079\n",
            "Epoch 32/100\n",
            "704/704 [==============================] - 19s 28ms/step - loss: 0.0065 - val_loss: 0.0079\n",
            "Epoch 33/100\n",
            "704/704 [==============================] - 26s 37ms/step - loss: 0.0064 - val_loss: 0.0081\n",
            "Epoch 34/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0063 - val_loss: 0.0092\n",
            "Epoch 35/100\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.0063 - val_loss: 0.0076\n",
            "Epoch 36/100\n",
            "704/704 [==============================] - 20s 28ms/step - loss: 0.0062 - val_loss: 0.0078\n",
            "Epoch 37/100\n",
            "704/704 [==============================] - 19s 27ms/step - loss: 0.0061 - val_loss: 0.0075\n",
            "Epoch 38/100\n",
            "704/704 [==============================] - 20s 29ms/step - loss: 0.0060 - val_loss: 0.0077\n",
            "Epoch 39/100\n",
            "704/704 [==============================] - 12s 16ms/step - loss: 0.0059 - val_loss: 0.0076\n",
            "Epoch 40/100\n",
            "704/704 [==============================] - 19s 27ms/step - loss: 0.0058 - val_loss: 0.0088\n",
            "Epoch 41/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0058 - val_loss: 0.0081\n",
            "Epoch 42/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0057 - val_loss: 0.0074\n",
            "Epoch 43/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0056 - val_loss: 0.0085\n",
            "Epoch 44/100\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0056 - val_loss: 0.0083\n",
            "Epoch 45/100\n",
            "704/704 [==============================] - 23s 33ms/step - loss: 0.0055 - val_loss: 0.0094\n",
            "Epoch 46/100\n",
            "704/704 [==============================] - 15s 21ms/step - loss: 0.0054 - val_loss: 0.0079\n",
            "Epoch 47/100\n",
            "704/704 [==============================] - 20s 28ms/step - loss: 0.0054 - val_loss: 0.0070\n",
            "Epoch 48/100\n",
            "704/704 [==============================] - 16s 22ms/step - loss: 0.0053 - val_loss: 0.0095\n",
            "Epoch 49/100\n",
            "704/704 [==============================] - 23s 33ms/step - loss: 0.0052 - val_loss: 0.0071\n",
            "Epoch 50/100\n",
            "704/704 [==============================] - 19s 27ms/step - loss: 0.0052 - val_loss: 0.0111\n",
            "Epoch 51/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0051 - val_loss: 0.0069\n",
            "Epoch 52/100\n",
            "704/704 [==============================] - 22s 31ms/step - loss: 0.0051 - val_loss: 0.0068\n",
            "Epoch 53/100\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0050 - val_loss: 0.0069\n",
            "Epoch 54/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0050 - val_loss: 0.0068\n",
            "Epoch 55/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0049 - val_loss: 0.0067\n",
            "Epoch 56/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0049 - val_loss: 0.0068\n",
            "Epoch 57/100\n",
            "704/704 [==============================] - 16s 23ms/step - loss: 0.0048 - val_loss: 0.0073\n",
            "Epoch 58/100\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0048 - val_loss: 0.0069\n",
            "Epoch 59/100\n",
            "704/704 [==============================] - 16s 23ms/step - loss: 0.0047 - val_loss: 0.0071\n",
            "Epoch 60/100\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.0047 - val_loss: 0.0065\n",
            "Epoch 61/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0046 - val_loss: 0.0071\n",
            "Epoch 62/100\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.0046 - val_loss: 0.0071\n",
            "Epoch 63/100\n",
            "704/704 [==============================] - 26s 37ms/step - loss: 0.0045 - val_loss: 0.0094\n",
            "Epoch 64/100\n",
            "704/704 [==============================] - 20s 29ms/step - loss: 0.0045 - val_loss: 0.0064\n",
            "Epoch 65/100\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.0045 - val_loss: 0.0064\n",
            "Epoch 66/100\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.0044 - val_loss: 0.0065\n",
            "Epoch 67/100\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0044 - val_loss: 0.0066\n",
            "Epoch 68/100\n",
            "704/704 [==============================] - 27s 38ms/step - loss: 0.0044 - val_loss: 0.0067\n",
            "Epoch 69/100\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0043 - val_loss: 0.0064\n",
            "Epoch 70/100\n",
            "704/704 [==============================] - 26s 37ms/step - loss: 0.0043 - val_loss: 0.0075\n",
            "Epoch 71/100\n",
            "704/704 [==============================] - 23s 32ms/step - loss: 0.0043 - val_loss: 0.0076\n",
            "Epoch 72/100\n",
            "704/704 [==============================] - 20s 29ms/step - loss: 0.0042 - val_loss: 0.0062\n",
            "Epoch 73/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0042 - val_loss: 0.0063\n",
            "Epoch 74/100\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0041 - val_loss: 0.0072\n",
            "Epoch 75/100\n",
            "704/704 [==============================] - 21s 29ms/step - loss: 0.0041 - val_loss: 0.0069\n",
            "Epoch 76/100\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0041 - val_loss: 0.0072\n",
            "Epoch 77/100\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0041 - val_loss: 0.0065\n",
            "Epoch 78/100\n",
            "704/704 [==============================] - 23s 33ms/step - loss: 0.0040 - val_loss: 0.0069\n",
            "Epoch 79/100\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0040 - val_loss: 0.0062\n",
            "Epoch 80/100\n",
            "704/704 [==============================] - 22s 31ms/step - loss: 0.0040 - val_loss: 0.0062\n",
            "Epoch 81/100\n",
            "704/704 [==============================] - 38s 54ms/step - loss: 0.0039 - val_loss: 0.0071\n",
            "Epoch 82/100\n",
            "704/704 [==============================] - 41s 59ms/step - loss: 0.0039 - val_loss: 0.0061\n",
            "Epoch 83/100\n",
            "704/704 [==============================] - 29s 41ms/step - loss: 0.0039 - val_loss: 0.0060\n",
            "Epoch 84/100\n",
            "704/704 [==============================] - 21s 30ms/step - loss: 0.0038 - val_loss: 0.0061\n",
            "Epoch 85/100\n",
            "704/704 [==============================] - 26s 38ms/step - loss: 0.0038 - val_loss: 0.0063\n",
            "Epoch 86/100\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.0038 - val_loss: 0.0067\n",
            "Epoch 87/100\n",
            "704/704 [==============================] - 21s 31ms/step - loss: 0.0038 - val_loss: 0.0060\n",
            "Epoch 88/100\n",
            "704/704 [==============================] - 34s 49ms/step - loss: 0.0037 - val_loss: 0.0082\n",
            "Epoch 89/100\n",
            "704/704 [==============================] - 22s 32ms/step - loss: 0.0037 - val_loss: 0.0064\n",
            "Epoch 90/100\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.0037 - val_loss: 0.0061\n",
            "Epoch 91/100\n",
            "704/704 [==============================] - 29s 42ms/step - loss: 0.0036 - val_loss: 0.0064\n",
            "Epoch 92/100\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0036 - val_loss: 0.0066\n",
            "Epoch 93/100\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.0036 - val_loss: 0.0059\n",
            "Epoch 94/100\n",
            "704/704 [==============================] - 19s 27ms/step - loss: 0.0036 - val_loss: 0.0064\n",
            "Epoch 95/100\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0036 - val_loss: 0.0062\n",
            "Epoch 96/100\n",
            "704/704 [==============================] - 20s 28ms/step - loss: 0.0035 - val_loss: 0.0058\n",
            "Epoch 97/100\n",
            "704/704 [==============================] - 23s 33ms/step - loss: 0.0035 - val_loss: 0.0060\n",
            "Epoch 98/100\n",
            "704/704 [==============================] - 14s 20ms/step - loss: 0.0035 - val_loss: 0.0059\n",
            "Epoch 99/100\n",
            "704/704 [==============================] - 19s 28ms/step - loss: 0.0034 - val_loss: 0.0060\n",
            "Epoch 100/100\n",
            "704/704 [==============================] - 19s 26ms/step - loss: 0.0034 - val_loss: 0.0081\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0cbd38a890>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.encoder.summary()\n",
        "autoencoder.predictor.summary()"
      ],
      "metadata": {
        "id": "wpWXediB3AcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predpretrain = autoencoder.predict(x_pretrain)\n"
      ],
      "metadata": {
        "id": "9DFper7t6D5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer learning to HOMO-LUMO gap learning\n",
        "# freeze encoder features and train predictor layer\n",
        "\n",
        "# run features through encoder\n",
        "encodedtrain = autoencoder.encoder(train).numpy()\n",
        "y_predicttrain = autoencoder.predict(train)"
      ],
      "metadata": {
        "id": "v4nCLebHLj6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "autoencoder.layers[-1].output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wnyNyyCyx47P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#hyperparameters\n",
        "dim = 1\n",
        "epochs = 100         # epoch = 100 loss = 0.181\n",
        "#batch_size = 64\n",
        "# create encoder and decoder to reduce dimensionality of features\n",
        "class Homolumo(Model):\n",
        "  def __init__(self, dim):\n",
        "    super(Homolumo, self).__init__()\n",
        "    self.dim = dim   \n",
        "    self.inflator = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(50, activation='relu'),\n",
        "      layers.Dense(70, activation='relu'),\n",
        "      layers.Dense(80, activation='relu'),\n",
        "      layers.Dense(100, activation='relu'),\n",
        "    ])\n",
        "    self.deflator = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(100, activation='relu'),\n",
        "      layers.Dense(80, activation='relu'),\n",
        "      layers.Dense(70, activation='relu'),\n",
        "      layers.Dense(50, activation='relu'),\n",
        "      layers.Dense(dim),\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    inflated = self.inflator(x)\n",
        "    deflated = self.deflator(inflated)\n",
        "    return deflated\n",
        "\n",
        "model = Homolumo(dim)\n",
        "\n",
        "sgd = SGD(learning_rate=0.1, decay=1e-2, momentum=0.9)\n",
        "\n",
        "model.compile(optimizer='sgd', loss=losses.MeanSquaredError())\n",
        "\n",
        "model.fit(y_predicttrain, train_labels, epochs=epochs, shuffle=True) #, validation_data=(y_predicttest, y_pretest))"
      ],
      "metadata": {
        "id": "rhUiKi4aIt6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.inflator.summary()\n",
        "model.deflator.summary()"
      ],
      "metadata": {
        "id": "21IjeLxFO9fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict homolumo gap for testset\n",
        "x_test = df_test.iloc[:,2::]\n",
        "id_test = df_test.iloc[:,0]\n",
        "\n",
        "y_predicttest = autoencoder.predict(x_test)\n",
        "y_test = model.predict(y_predicttest)"
      ],
      "metadata": {
        "id": "TZpt5Lz7MQBF"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export solution\n",
        "df_evaltest = pd.DataFrame(y_test, columns = ['y'])\n",
        "df_evaltest.insert(0, 'Id', id_test)\n",
        "df_evaltest.to_csv('solutiontot.csv', index=False, index_label=False, encoding = 'utf-8')"
      ],
      "metadata": {
        "id": "SUwPuLt_PiIm"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_evaltest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abHf365lqgeb",
        "outputId": "8999d33e-2703-4235-d4c2-1b2fa245d59c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Id         y\n",
            "0     50100  1.751833\n",
            "1     50101  1.763083\n",
            "2     50102  1.838370\n",
            "3     50103  1.667388\n",
            "4     50104  1.844060\n",
            "...     ...       ...\n",
            "9995  60095  1.655768\n",
            "9996  60096  1.870003\n",
            "9997  60097  1.908120\n",
            "9998  60098  1.717433\n",
            "9999  60099  1.862450\n",
            "\n",
            "[10000 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}